# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Functional smoke tests for Nemotron Nano v2 finetuning recipe configurations."""

import json
import os
import shutil
import sys

import pytest
import torch
from transformers import AutoConfig, AutoTokenizer
from transformers.dynamic_module_utils import get_class_from_dynamic_module

from megatron.bridge.models.conversion.auto_bridge import AutoBridge
from megatron.bridge.recipes.nemotronh import (
    nemotron_nano_9b_v2_finetune_config,
)


# Overrides for toy model - create a tiny version that matches Nemotron Nano v2 architecture
# Key: keep the same mamba_num_heads and mamba_head_dim ratio as the full model for shape compatibility
HF_NEMOTRONH_TOY_MODEL_OVERRIDES = {
    "attention_head_dim": 80,  # Match mamba_head_dim
    "head_dim": 80,  # Match mamba_head_dim
    "chunk_size": 48,
    "expand": 2,
    "hidden_size": 640,  # 8 heads * 80 head_dim
    "hybrid_override_pattern": "M*M-",  # Minimal 4-layer pattern
    "initializer_range": 0.02,
    "intermediate_size": 2240,  # ~3.5x hidden_size
    "layer_norm_epsilon": 1e-05,
    "mamba_head_dim": 80,  # Match Nemotron Nano v2
    "mamba_hidden_act": "silu",
    "mamba_num_heads": 128,  # Match Nemotron Nano v2 for shape compatibility
    "max_position_embeddings": 8192,
    "n_groups": 8,
    "num_attention_heads": 8,  # hidden_size / attention_head_dim = 640 / 80
    "num_hidden_layers": 4,
    "num_key_value_heads": 4,  # Half of num_attention_heads (GQA)
    "ssm_state_size": 128,  # Match Nemotron Nano v2
    "vocab_size": 131072,
}


class TestNemotronNanoV2FinetuneRecipes:
    """Test class for Nemotron Nano v2 finetune recipe functional tests."""

    @pytest.fixture(scope="class")
    def nemotronh_toy_model_path(self, tmp_path_factory):
        """
        Create and save a HuggingFace NemotronH toy model from config to a temporary directory.

        Args:
            tmp_path_factory: Pytest temporary path factory for class-scoped fixtures

        Returns:
            str: Path to the saved HuggingFace model directory
        """
        # Create a temporary directory for this test class
        temp_dir = tmp_path_factory.mktemp("nemotronh_toy_model")
        model_dir = temp_dir / "nemotronh_toy"

        # Create NemotronH toy model config by starting with 8B and applying overrides
        config = AutoConfig.from_pretrained("nvidia/NVIDIA-Nemotron-Nano-9B-v2", trust_remote_code=True)
        for k, v in HF_NEMOTRONH_TOY_MODEL_OVERRIDES.items():
            setattr(config, k, v)

        # Create model with random weights and convert to bfloat16
        model_class_ref = config.auto_map["AutoModelForCausalLM"]
        model_class = get_class_from_dynamic_module(
            class_reference=model_class_ref,
            pretrained_model_name_or_path="nvidia/NVIDIA-Nemotron-Nano-9B-v2",
            cache_dir=None,
            force_download=False,
            resume_download=True,
            proxies=None,
            use_auth_token=None,
            revision=None,
            local_files_only=False,
            repo_id="nvidia/NVIDIA-Nemotron-Nano-9B-v2",
        )
        model = model_class(config)
        model = model.bfloat16() if hasattr(model, "bfloat16") else model

        # Download and save tokenizer from a reference NemotronH model
        tokenizer = AutoTokenizer.from_pretrained("nvidia/NVIDIA-Nemotron-Nano-9B-v2", trust_remote_code=True)
        tokenizer.save_pretrained(model_dir)

        # Save model, config, and modeling code to directory
        model.save_pretrained(model_dir, safe_serialization=True)
        modeling_filepath = os.path.abspath(sys.modules[model_class.__module__].__file__)
        shutil.copy(modeling_filepath, model_dir)

        # Ensure config.json exists with expected keys
        config_path = model_dir / "config.json"
        with open(config_path, "w") as f:
            json.dump(model.config.to_dict(), f, indent=2)

        return str(model_dir)

    @pytest.fixture(scope="class")
    def nemotronh_megatron_checkpoint(self, nemotronh_toy_model_path, tmp_path_factory):
        """
        Convert the toy HuggingFace model to Megatron checkpoint format.

        Args:
            nemotronh_toy_model_path: Path to the toy HuggingFace model (from fixture)
            tmp_path_factory: Pytest temporary path factory for class-scoped fixtures

        Returns:
            str: Path to the Megatron checkpoint directory
        """
        from megatron.core import parallel_state
        from megatron.core.rerun_state_machine import destroy_rerun_state_machine

        # Create a temporary directory for the Megatron checkpoint
        temp_dir = tmp_path_factory.mktemp("nemotronh_megatron_ckpt")
        megatron_checkpoint_dir = temp_dir / "megatron_checkpoint"

        # Import the HF model to Megatron format
        AutoBridge.import_ckpt(
            hf_model_id=nemotronh_toy_model_path,
            megatron_path=str(megatron_checkpoint_dir),
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
        )

        # Clean up global state after import_ckpt
        # This is necessary to avoid conflicts when the actual test initializes Megatron
        if parallel_state.model_parallel_is_initialized():
            parallel_state.destroy_model_parallel()
        destroy_rerun_state_machine()

        return str(megatron_checkpoint_dir)

    def _finetune_wrapper_lora(self, checkpoint_dir, **kwargs):
        """Wrapper to adapt Nemotron Nano v2 finetune_config to the test runner signature (with LoRA).

        The runner will pass (dir, name) among others; we forward
        everything to finetune_config and inject the toy model checkpoint.
        """
        kwargs.setdefault("pretrained_checkpoint", checkpoint_dir)
        kwargs.setdefault("peft", "lora")  # Explicitly use LoRA
        return nemotron_nano_9b_v2_finetune_config(**kwargs)

    def _finetune_wrapper_full(self, checkpoint_dir, **kwargs):
        """Wrapper to adapt Nemotron Nano v2 finetune_config to the test runner signature (full SFT, no LoRA).

        The runner will pass (dir, name) among others; we forward
        everything to finetune_config and inject the toy model checkpoint.
        """
        kwargs.setdefault("pretrained_checkpoint", checkpoint_dir)
        kwargs.setdefault("peft", None)  # No PEFT for full finetuning
        return nemotron_nano_9b_v2_finetune_config(**kwargs)

    @pytest.mark.run_only_on("GPU")
    @pytest.mark.parametrize(
        "recipe_name,model_overrides,use_lora",
        [
            (
                "nemotron_nano_9b_v2_lora",
                {
                    "num_layers": 4,  # Match toy model
                    "hybrid_override_pattern": "M*M-",  # Match toy model
                    "hidden_size": 640,  # Match toy model
                    "ffn_hidden_size": 2240,  # Match toy model
                    "num_attention_heads": 8,  # Match toy model
                    "kv_channels": 80,  # Match toy model attention_head_dim
                    "num_query_groups": 4,  # Match toy model
                    "mamba_num_heads": 128,  # Match toy model
                    "mamba_head_dim": 80,  # Match toy model
                    "mamba_state_dim": 128,  # Match toy model
                    "tensor_model_parallel_size": 1,
                    "pipeline_model_parallel_size": 1,
                    "sequence_parallel": False,
                },
                True,
            ),
            (
                "nemotron_nano_9b_v2_full",
                {
                    "num_layers": 4,  # Match toy model
                    "hybrid_override_pattern": "M*M-",  # Match toy model
                    "hidden_size": 640,  # Match toy model
                    "ffn_hidden_size": 2240,  # Match toy model
                    "num_attention_heads": 8,  # Match toy model
                    "kv_channels": 80,  # Match toy model attention_head_dim
                    "num_query_groups": 4,  # Match toy model
                    "mamba_num_heads": 128,  # Match toy model
                    "mamba_head_dim": 80,  # Match toy model
                    "mamba_state_dim": 128,  # Match toy model
                    "tensor_model_parallel_size": 1,
                    "pipeline_model_parallel_size": 1,
                    "sequence_parallel": False,
                },
                False,
            ),
        ],
    )
    def test_nemotron_nano_v2_finetune_recipes(
        self, nemotronh_megatron_checkpoint, recipe_name, model_overrides, use_lora, tmp_path
    ):
        """Functional test for Nemotron Nano v2 finetuning recipes with LoRA and full SFT."""
        # Create the config using the appropriate wrapper
        if use_lora:
            config = self._finetune_wrapper_lora(
                checkpoint_dir=nemotronh_megatron_checkpoint, dir=str(tmp_path), name=recipe_name
            )
        else:
            config = self._finetune_wrapper_full(
                checkpoint_dir=nemotronh_megatron_checkpoint, dir=str(tmp_path), name=recipe_name
            )

        # Override the dataset to use MockGPTDataset for faster testing
        from megatron.bridge.training.config import MockGPTDatasetConfig

        seq_length = 512
        config.dataset = MockGPTDatasetConfig(
            random_seed=5678,
            reset_attention_mask=False,
            reset_position_ids=False,
            eod_mask_loss=False,
            sequence_length=seq_length,
            num_dataset_builder_threads=1,
            data_sharding=True,
            dataloader_type="single",
            num_workers=0,
        )

        # Apply model overrides
        for attribute_name, attribute_value in model_overrides.items():
            setattr(config.model, attribute_name, attribute_value)

        # Override to use smaller model for faster testing
        config.model.seq_length = seq_length
        config.train.train_iters = 10
        config.train.eval_interval = 5
        config.train.eval_iters = 2
        config.train.micro_batch_size = 1
        config.train.global_batch_size = 8
        config.scheduler.lr_warmup_iters = 2

        # Calculate proper dataset splits
        train_samples_needed = config.train.train_iters * config.train.global_batch_size
        eval_samples_needed = config.train.eval_iters * config.train.global_batch_size
        test_samples_needed = 100
        total_samples = train_samples_needed + eval_samples_needed + test_samples_needed

        train_split = train_samples_needed / total_samples
        valid_split = eval_samples_needed / total_samples
        test_split = test_samples_needed / total_samples
        config.dataset.split = [train_split, valid_split, test_split]

        # Run the test using the actual finetuning function
        from megatron.bridge.training.finetune import finetune
        from megatron.bridge.training.gpt_step import forward_step
        from tests.functional_tests.utils import (
            clear_directories,
            initialize_distributed,
            verify_checkpoint_files,
        )

        initialize_distributed()
        try:
            # Run finetuning
            finetune(config, forward_step)

            # Verify checkpoints were saved
            verify_checkpoint_files(config.checkpoint.save, config.train.train_iters)

        finally:
            clear_directories(tmp_path)
